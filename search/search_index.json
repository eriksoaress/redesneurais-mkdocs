{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Entregas Redes Neurais","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#erik-soares","title":"Erik Soares","text":""},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Data - https://eriksoaress.github.io/redesneurais-mkdocs/data/main/</li> <li> Perceptron - https://eriksoaress.github.io/redesneurais-mkdocs/perceptron/main/</li> <li> MLP - https://eriksoaress.github.io/redesneurais-mkdocs/mlp/main/</li> <li> Metrics</li> </ul>"},{"location":"index_template/","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"index_template/#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"index_template/#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"index_template/#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"index_template/#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"index_template/#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"index_template/#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"data/main/","title":"Redes Neurais - Dados","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"data/main/#erik-soares","title":"Erik Soares","text":""},{"location":"data/main/#exercicio-1","title":"Exerc\u00edcio 1","text":""},{"location":"data/main/#explorando-a-separabilidade-de-classes-em-2d","title":"Explorando a Separabilidade de Classes em 2D","text":""},{"location":"data/main/#instrucoes","title":"Instru\u00e7\u00f5es","text":"<ol> <li> <p>Gerar os Dados: Crie um conjunto de dados sint\u00e9tico com um total de 400 amostras, divididas igualmente entre 4 classes (100 amostras cada). Utilize uma distribui\u00e7\u00e3o Gaussiana para gerar os pontos para cada classe com base nos seguintes par\u00e2metros:</p> <ul> <li>Classe 0: M\u00e9dia = [2, 3], Desvio Padr\u00e3o = [0.8, 2.5]</li> <li>Classe 1: M\u00e9dia = [5, 6], Desvio Padr\u00e3o = [1.2, 1.9]</li> <li>Classe 2: M\u00e9dia = [8, 1], Desvio Padr\u00e3o = [0.9, 0.9]</li> <li>Classe 3: M\u00e9dia = [15, 4], Desvio Padr\u00e3o = [0.5, 2.0]</li> </ul> </li> <li> <p>Plotar os Dados: Crie um gr\u00e1fico de dispers\u00e3o 2D mostrando todos os pontos de dados. Utilize uma cor diferente para cada classe para torn\u00e1-las distingu\u00edveis.</p> </li> <li> <p>Analisar e Desenhar Limites: </p> </li> <li>Examine o gr\u00e1fico de dispers\u00e3o com aten\u00e7\u00e3o. Descreva a distribui\u00e7\u00e3o e a sobreposi\u00e7\u00e3o das quatro classes.</li> <li>Com base na sua inspe\u00e7\u00e3o visual, um limite simples e linear poderia separar todas as classes?</li> <li>Em seu gr\u00e1fico, esboce os limites de decis\u00e3o que voc\u00ea acha que uma rede neural treinada poderia aprender para separar essas classes.</li> </ol>"},{"location":"data/main/#solucao","title":"Solu\u00e7\u00e3o","text":""},{"location":"data/main/#geracao-e-plotagem-dos-dados","title":"Gera\u00e7\u00e3o e Plotagem dos Dados","text":"<p>O c\u00f3digo abaixo gera os dados sint\u00e9ticos conforme as especifica\u00e7\u00f5es e os plota em um gr\u00e1fico de dispers\u00e3o 2D.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nparams = {\n    'classe_0': {'media': [2, 3], 'desvio_padrao': [0.8, 2.5], 'n_amostras': 100},\n    'classe_1': {'media': [5, 6], 'desvio_padrao': [1.2, 1.9], 'n_amostras': 100},\n    'classe_2': {'media': [8, 1], 'desvio_padrao': [0.9, 0.9], 'n_amostras': 100},\n    'classe_3': {'media': [15, 4], 'desvio_padrao': [0.5, 2.0], 'n_amostras': 100}\n}\n\nX = []\ny = []\n\nfor i, (classe, p) in enumerate(params.items()):\n    dados_classe = np.random.normal(\n        loc=p['media'],\n        scale=p['desvio_padrao'],\n        size=(p['n_amostras'], 2),\n    )\n\n    X.append(dados_classe)\n    y.append(np.full(p['n_amostras'], fill_value=i))\n\nX = np.concatenate(X)\ny = np.concatenate(y)\n\nplt.figure(figsize=(12, 8))\n\ncores = ['blue', 'orange', 'green', 'red']\nlabels_classes = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3']\n\nfor i in range(4):\n    plt.scatter(\n        X[y == i, 0],\n        X[y == i, 1],\n        c=cores[i],\n        label=labels_classes[i],\n        edgecolors='k'\n    )\n\n\nplt.title('Distribui\u00e7\u00e3o das 4 Classes de Dados Sint\u00e9ticos', fontsize=16)\nplt.xlabel('Caracter\u00edstica 1 (X)', fontsize=12)\nplt.ylabel('Caracter\u00edstica 2 (Y)', fontsize=12)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.show()\n</code></pre> <p></p>"},{"location":"data/main/#analise-e-limites-de-decisao","title":"An\u00e1lise e Limites de Decis\u00e3o","text":"<p>Ap\u00f3s a execu\u00e7\u00e3o do c\u00f3digo acima, ser\u00e1 gerado um gr\u00e1fico de dispers\u00e3o. A an\u00e1lise visual do gr\u00e1fico revela:</p> <ul> <li>Classe 0: A classe 0 \u00e9 a classe que apresenta os dados mais dispersos, principalmente em rela\u00e7\u00e3o a caracter\u00edstica 2 (Y), alguns dados se aproximam e se misturam com dados da classe 1.</li> <li>Classe 1: A classe 1 est\u00e1 localizada acima e a direita da classe 1, mas n\u00e3o t\u00e3o distante, o que permite uma mistura consider\u00e1vel de seus dados, al\u00e9m disso, poucos dados ainda conseguem se misturar com a classe 2. Essa classe tamb\u00e9m apresenta uma certa dispers\u00e3o, em rela\u00e7\u00e3o ao componente 1 e 2, mas ainda menor que a classe 1.</li> <li>Classe 2: A classe dois est\u00e1 localizada a direita da classe 0 e 1, e um pouco para baixo, ela est\u00e1 mais afastada e por isso tem uma mistura m\u00ednima com as outras classes. \u00c9 uma classe com uma dispers\u00e3o bem baixa em rela\u00e7\u00e3o a ambos os componentes.</li> <li>Classe 3: A classe 3 \u00e9 a classe mais isolada, nenhum de seus dados se misturam com outras classes, est\u00e1 totalmente \u00e0 direita. Sua dispers\u00e3o \u00e9 predominantemente em rela\u00e7\u00e3o a caracter\u00edstica 2 (Y).</li> </ul> <p>As classes 0, 1 e 2 parecem ser razoavelmente separ\u00e1veis linearmente, embora possa haver alguma sobreposi\u00e7\u00e3o entre elas. A Classe 3 est\u00e1 bem distante das outras, indicando uma alta separabilidade. Portanto, um limite simples e linear n\u00e3o \u00e9 capaz de separar adequadamente todas as classes, pois h\u00e1 classes que se misturam, e at\u00e9 mesmo pontos de classes diferentes que est\u00e3o praticamente um sob o outro.</p>"},{"location":"data/main/#simulacao-dos-limites-de-decisao","title":"Simula\u00e7\u00e3o dos limites de decis\u00e3o","text":"<p>O c\u00f3digo abaixo esbo\u00e7a os limites de decis\u00e3o lineares que tenta separar as classes: <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nparams = {\n    'classe_0': {'media': [2, 3], 'desvio_padrao': [0.8, 2.5], 'n_amostras': 100},\n    'classe_1': {'media': [5, 6], 'desvio_padrao': [1.2, 1.9], 'n_amostras': 100},\n    'classe_2': {'media': [8, 1], 'desvio_padrao': [0.9, 0.9], 'n_amostras': 100},\n    'classe_3': {'media': [15, 4], 'desvio_padrao': [0.5, 2.0], 'n_amostras': 100}\n}\n\nX = []\ny = []\n\nfor i, (classe, p) in enumerate(params.items()):\n    dados_classe = np.random.normal(\n        loc=p['media'],\n        scale=p['desvio_padrao'],\n        size=(p['n_amostras'], 2),\n    )\n\n    X.append(dados_classe)\n    y.append(np.full(p['n_amostras'], fill_value=i))\n\nX = np.concatenate(X)\ny = np.concatenate(y)\n\nx_line1 = np.array([0, 12])\ny_line1 = -2.9 * x_line1 + 16\nplt.plot(x_line1, y_line1, '-', color='k', lw=2)\n\n\nx_line2 = np.array([2, 12])\ny_line2 = 1.3 * x_line2 - 6\nplt.plot(x_line2, y_line2, '-', color='k', lw=2)\n\nplt.axvline(x=12, linestyle='-', color='k', lw=2)\n\nfor i in range(4):\n    plt.scatter(\n        X[y == i, 0], X[y == i, 1], \n        color=cores[i], \n        edgecolors='k', \n        label=labels_classes[i],\n    )\nplt.title('Distribui\u00e7\u00e3o com Limites de Decis\u00e3o Lineares (Fun\u00e7\u00f5es Afins)', fontsize=16)\nplt.xlabel('Caracter\u00edstica 1 (X)', fontsize=12)\nplt.ylabel('Caracter\u00edstica 2 (Y)', fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\n\nplt.xlim(-1, 17)\nplt.ylim(-3, 14)\n\nplt.show()\n</code></pre> </p>"},{"location":"data/main/#exercicio-2","title":"Exerc\u00edcio 2","text":""},{"location":"data/main/#nao-linearidade-em-dimensoes-superiores","title":"N\u00e3o Linearidade em Dimens\u00f5es Superiores","text":""},{"location":"data/main/#instrucoes_1","title":"Instru\u00e7\u00f5es","text":"<p>Redes neurais simples (como um Perceptron) s\u00f3 podem aprender limites lineares. Redes profundas se destacam quando os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis. Este exerc\u00edcio desafia voc\u00ea a criar e visualizar um conjunto de dados n\u00e3o linearmente separ\u00e1vel.</p> <ol> <li> <p>Gerar os Dados: Crie um conjunto de dados com 500 amostras para a Classe A e 500 amostras para a Classe B. Utilize uma distribui\u00e7\u00e3o normal multivariada com os seguintes par\u00e2metros:</p> <ul> <li> <p>Classe A:</p> <ul> <li>Vetor m\u00e9dio:      <pre><code>\u03bc_A = [0, 0, 0, 0, 0]\n</code></pre></li> <li>Matriz de covari\u00e2ncia:     <pre><code>\u03a3_A = [[1.0, 0.8, 0.1, 0.0, 0.0],\n       [0.8, 1.0, 0.3, 0.0, 0.0],\n       [0.1, 0.3, 1.0, 0.5, 0.0],\n       [0.0, 0.0, 0.5, 1.0, 0.2],\n       [0.0, 0.0, 0.0, 0.2, 1.0]]\n</code></pre></li> </ul> </li> <li> <p>Classe B:</p> <ul> <li>Vetor m\u00e9dio:      <pre><code>\u03bc_B = [1.5, 1.5, 1.5, 1.5, 1.5]\n</code></pre></li> <li>Matriz de covari\u00e2ncia:     <pre><code>\u03a3_B = [[1.5, -0.7, 0.2, 0.0, 0.0],\n       [-0.7, 1.5, 0.4, 0.0, 0.0],\n       [0.2, 0.4, 1.5, 0.6, 0.0],\n       [0.0, 0.0, 0.6, 1.5, 0.3],\n       [0.0, 0.0, 0.0, 0.3, 1.5]]\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Redu\u00e7\u00e3o de Dimensionalidade (PCA): </p> </li> <li> <p>Como os dados est\u00e3o em 5 dimens\u00f5es, utilize a An\u00e1lise de Componentes Principais (PCA) para reduzir a dimensionalidade para 2 componentes principais. Isso permitir\u00e1 a visualiza\u00e7\u00e3o.</p> </li> <li> <p>Crie um gr\u00e1fico de dispers\u00e3o 2D dos dados transformados pela PCA. Utilize uma cor diferente para cada classe.</p> </li> <li> <p>Analisar a Separabilidade: Com base no gr\u00e1fico de dispers\u00e3o, discuta se as classes s\u00e3o linearmente separ\u00e1veis em 2D. Explique por que a n\u00e3o linearidade \u00e9 importante para redes neurais profundas neste contexto.</p> </li> </ol>"},{"location":"data/main/#solucao_1","title":"Solu\u00e7\u00e3o","text":""},{"location":"data/main/#geracao-de-dados-e-reducao-de-dimensionalidade","title":"Gera\u00e7\u00e3o de Dados e Redu\u00e7\u00e3o de Dimensionalidade","text":"<p>O c\u00f3digo abaixo gera os dados para as Classes A e B, e ent\u00e3o aplica PCA para reduzir a dimensionalidade para 2D.</p> <p><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\n\njn_amostras_A= 500\nn_amostras_B = 500\n\nmedia_A = np.array([0, 0, 0, 0, 0])\nmedia_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\n\ncovariancia_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\ncovariancia_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\nX = np.vstack((dados_A, dados_B))\ny = np.hstack((np.zeros(dados_A.shape[0]), np.ones(dados_B.shape[0])))\n\npca = PCA(n_components=2)\n\ndados_2D = pca.fit_transform(X)\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 8))\n\nplt.scatter(\n    dados_2D[y == 0, 0],\n    dados_2D[y == 0, 1],  \n    c='blue',\n    label='Classe A',\n    alpha=0.7\n)\n\nplt.scatter(\n    dados_2D[y == 1, 0],\n    dados_2D[y == 1, 1],\n    c='red',\n    label='Classe B',\n    alpha=0.7\n)\n\nplt.title('Visualiza\u00e7\u00e3o 2D dos Dados com An\u00e1lise de Componentes Principais (PCA)', fontsize=15)\nplt.xlabel('Primeiro Componente Principal (PC1)', fontsize=12)\nplt.ylabel('Segundo Componente Principal (PC2)', fontsize=12)\nplt.legend(fontsize=12)\nplt.show()\n</code></pre> </p>"},{"location":"data/main/#analise-da-separabilidade","title":"An\u00e1lise da Separabilidade","text":"<p>Ao observar o gr\u00e1fico gerado, \u00e9 evidente que as duas classes (A e B) se sobrep\u00f5em significativamente no espa\u00e7o 2D ap\u00f3s a redu\u00e7\u00e3o de dimensionalidade via PCA. Isso indica que os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis neste espa\u00e7o. Uma \u00fanica linha reta n\u00e3o seria capaz de dividir as duas classes de forma eficaz.</p> <p>Import\u00e2ncia da N\u00e3o Linearidade para Redes Neurais Profundas:</p> <ul> <li> <p>Perceptrons Simples: Um Perceptron simples \u00e9 um classificador linear. Ele s\u00f3 pode aprender limites de decis\u00e3o que s\u00e3o linhas (em 2D), planos (em 3D) ou hiperplanos (em dimens\u00f5es superiores). Se os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis, um Perceptron simples n\u00e3o conseguir\u00e1 classific\u00e1-los corretamente.</p> </li> <li> <p>Redes Neurais Profundas: Redes neurais profundas, com suas m\u00faltiplas camadas ocultas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares, s\u00e3o capazes de aprender e modelar rela\u00e7\u00f5es complexas e n\u00e3o lineares nos dados. Cada camada oculta pode transformar os dados em uma nova representa\u00e7\u00e3o, onde as classes podem se tornar linearmente separ\u00e1veis. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares (como ReLU, sigmoid, tanh) s\u00e3o cruciais para essa capacidade, pois introduzem a n\u00e3o linearidade necess\u00e1ria para aprender limites de decis\u00e3o complexos e curvos.</p> </li> </ul> <p>Neste exerc\u00edcio, a sobreposi\u00e7\u00e3o das classes no espa\u00e7o 2D demonstra a necessidade de um modelo mais complexo do que um classificador linear. Redes neurais profundas, com sua arquitetura multicamadas e n\u00e3o linearidades, s\u00e3o ideais para lidar com esse tipo de problema, permitindo a identifica\u00e7\u00e3o de padr\u00f5es e a separa\u00e7\u00e3o de classes que n\u00e3o s\u00e3o trivialmente distingu\u00edveis por uma fronteira linear.</p>"},{"location":"data/main/#exercicio-3","title":"Exerc\u00edcio 3","text":""},{"location":"data/main/#preparando-dados-do-mundo-real-para-uma-rede-neural","title":"Preparando Dados do Mundo Real para uma Rede Neural","text":""},{"location":"data/main/#instrucoes_2","title":"Instru\u00e7\u00f5es","text":"<p>Este exerc\u00edcio utiliza um conjunto de dados real do Kaggle. Sua tarefa \u00e9 realizar o pr\u00e9-processamento necess\u00e1rio para torn\u00e1-lo adequado para uma rede neural que utiliza a tanhfun\u00e7\u00e3o de ativa\u00e7\u00e3o tangente hiperb\u00f3lica ( ) em suas camadas ocultas.</p> <ol> <li>Obtenha os Dados: baixe o conjunto de dados da nave espacial Titanic do Kaggle.</li> <li>Descreva os dados: </li> <li>Descreva brevemente o objetivo do conjunto de dados (ou seja, o que a Transportedcoluna representa?).</li> <li>Liste as caracter\u00edsticas e identifique quais s\u00e3o num\u00e9ricas (por exemplo, Age, RoomService) e quais s\u00e3o categ\u00f3ricas (por exemplo, HomePlanet, Destination).</li> <li>Investigue o conjunto de dados em busca de valores ausentes . Quais colunas os cont\u00eam e quantos?</li> <li>Pr\u00e9-processar os Dados: Seu objetivo \u00e9 limpar e transformar os dados para que possam ser alimentados em uma rede neural. A tanhfun\u00e7\u00e3o de ativa\u00e7\u00e3o produz sa\u00eddas no intervalo [-1, 1], portanto, seus dados de entrada devem ser dimensionados adequadamente para um treinamento est\u00e1vel.<ul> <li>Tratamento de valores ausentes (ex: imputa\u00e7\u00e3o, remo\u00e7\u00e3o).</li> <li>Codifica\u00e7\u00e3o de features categ\u00f3ricas (ex: one-hot encoding, label encoding).</li> <li>Escalonamento de features (ex: MinMaxScaler, StandardScaler).</li> </ul> </li> <li>Visualize os resultados: Crie histogramas para um ou dois recursos num\u00e9ricos (como FoodCourtou Age) antes e depois do dimensionamento para mostrar o efeito da sua transforma\u00e7\u00e3o.</li> </ol>"},{"location":"data/main/#solucao_2","title":"Solu\u00e7\u00e3o","text":""},{"location":"data/main/#carregamento-exploracao-e-tratamento-dos-dados","title":"Carregamento, explora\u00e7\u00e3o e tratamento dos dados","text":"<p>O c\u00f3digo abaixo \u00e9 respons\u00e1vel por carregar, explorar os dados, e tratar tudo o que for necess\u00e1rio para a utiliza\u00e7\u00e3o de um modelo de rede neural.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp.random.seed(42)\n\n# Carregar o dataset\ndf = pd.read_csv(\"./spaceship-titanic/train.csv\")\n\ncontagem_nulos = df.isnull().sum()\nprint(f'Nulos: {contagem_nulos}')\nprint(df.describe())\n\n\ndf['HomePlanet'] = df['HomePlanet'].fillna('Earth') # Foi preenchido com Earth pois \u00e9 o valor predominante (54%)\n\ndf['CryoSleep'] = df['CryoSleep'].fillna(False) # Foi preenchido com False pois \u00e9 o valor predominante (64%)\n\ndf = df.dropna(subset=['Cabin']) # Remove os nulos de cabin, pois n\u00e3o h\u00e1 predomin\u00e2ncia de um valor que possa ser embutido e que n\u00e3o prejudique a an\u00e1lise, como m\u00e9dia, mediana, etc.\n# Al\u00e9m disso, 199 n\u00e3o \u00e9 uma quantidade significativa para nosso tamanho de dataset\n\ndf[['cabin_deck', 'cabin_num', 'cabin_side']] = df['Cabin'].str.split('/', expand=True)\ndf = df.drop(columns=['Cabin'])\n\ndf['Destination'] = df['Destination'].fillna('TRAPPIST-1e') # TRAPPIST-1e representa 69% dos dados, podemos utiliz\u00e1-lo para preencher os nulos\n\nmediana_age = df['Age'].median()\ndf['Age'] = df['Age'].fillna(mediana_age) # Preenchemos com a mediana para sermos resistentes \u00e0 outliers\n\ndf['VIP'] = df['VIP'].fillna(False) # Os n\u00e3o VIPS representam 97% do dataset.\n\n# Os valores utilizados no preenchimento representam cerca de 60% ou mais das linhas, \u00e9 melhor do que pegarmos a m\u00e9dia.\ndf['RoomService'] = df['RoomService'].fillna(0)\ndf['FoodCourt'] = df['FoodCourt'].fillna(0)\ndf['ShoppingMall'] = df['ShoppingMall'].fillna(0)\ndf['Spa'] = df['Spa'].fillna(0)\ndf['VRDeck'] = df['VRDeck'].fillna(0)\n\n# Como a coluna Name n\u00e3o ser\u00e1 utilizada pelo nosso modelo por n\u00e3o conter nenhuma informa\u00e7\u00e3o relevante podemos manter os registros, e apenas apagar a coluna futuramente.\n\n# Transformando dados categ\u00f3ricos em valores 0 e 1:\ndf = pd.get_dummies(df, columns=['HomePlanet', 'Destination', 'cabin_deck', 'cabin_side'])\ncolunas_booleanas = df.select_dtypes(include=['bool', 'boolean']).columns\nfor coluna in colunas_booleanas:\n    df[coluna] = df[coluna].astype('Int64')\ndf = df.drop(columns=['PassengerId', 'Name'])\n\n# Utilizando MinMaxScaler para manter os dados em uma escala de -1 a 1:\nscaler = MinMaxScaler(feature_range=(-1, 1))\ncolunas_numericas = df.select_dtypes(include=np.number).columns\ndf_normalizado = df.copy()\ndf_normalizado[colunas_numericas] = scaler.fit_transform(df_normalizado[colunas_numericas])\n\n\ncolunas_para_visualizar = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\nfig, axes = plt.subplots(nrows=len(colunas_para_visualizar), ncols=2, figsize=(12, 10))\n\nfig.suptitle('Histogramas Antes e Depois da Normaliza\u00e7\u00e3o', fontsize=16)\n\nfor i, col in enumerate(colunas_para_visualizar):\n    # Plotar o histograma dos dados originais (\"Antes\")\n    axes[i, 0].hist(df[col], bins=20, color='skyblue', edgecolor='black')\n    axes[i, 0].set_title(f'{col} (Antes da Normaliza\u00e7\u00e3o)')\n    axes[i, 0].set_xlabel('Valor Original')\n    axes[i, 0].set_ylabel('Frequ\u00eancia')\n\n    # Plotar o histograma dos dados normalizados (\"Depois\")\n    axes[i, 1].hist(df_normalizado[col], bins=20, color='salmon', edgecolor='black')\n    axes[i, 1].set_title(f'{col} (Depois da Normaliza\u00e7\u00e3o)')\n    axes[i, 1].set_xlabel('Valor Normalizado [-1, 1]')\n    axes[i, 1].set_ylabel('Frequ\u00eancia')\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])\n\nplt.show()\n</code></pre>"},{"location":"data/main/#pre-processamento-dos-dados-e-justificativa-das-escolhas","title":"Pr\u00e9-processamento dos Dados e Justificativa das Escolhas","text":"<p>Para preparar os dados para uma rede neural, aplicamos as seguintes etapas de pr\u00e9-processamento:</p> <ol> <li> <p>Tratamento de Valores Ausentes:</p> <ul> <li><code>HomePlanet</code>: Imputamos os valores ausentes com o valor 'Earth', pois \u00e9 o valor predominante.</li> <li><code>CryoSleep</code>: Imputamos os valores ausentes com o valor False, pois \u00e9 o valor predominante.</li> <li><code>Embarked</code>: Imputamos os valores ausentes com a moda (valor mais frequente), pois \u00e9 uma feature categ\u00f3rica.</li> <li><code>Cabin</code>: Removemos os registros em que essa coluna tinha valores nulos, pois seria dif\u00edcil utilizar algum tipo de m\u00e9trica que n\u00e3o atrapalhasse nosso modelo. Al\u00e9m disso separamos essa coluna em 3: 'cabin_deck', 'cabin_num', 'cabin_side'.</li> <li><code>Destination</code>: Imputamos os valores ausentes com o valor 'TRAPPIST-1e', pois \u00e9 o valor predominante. </li> <li><code>Age</code>: Imputamos os valores ausentes com a mediana para sermos resistentes \u00e0 outliers. </li> <li><code>VIP</code>: Imputamos os valores ausentes com o valor False, pois representam 97% do dataset.</li> <li><code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>: Imputamos os valores ausentes com o valor 0, pois \u00e9 o valor predominante.</li> </ul> </li> <li> <p>Codifica\u00e7\u00e3o de Features Categ\u00f3ricas:</p> <ul> <li>Aplicamos One-Hot Encoding para converter essas features categ\u00f3ricas em representa\u00e7\u00f5es num\u00e9ricas bin\u00e1rias. Isso \u00e9 crucial para redes neurais, que operam com dados num\u00e9ricos.</li> </ul> </li> <li> <p>Escalonamento de Features Num\u00e9ricas:</p> <ul> <li>Aplicamos <code>MinMaxScaler</code> para normalizar essas features para um intervalo entre -1 e 1. O escalonamento \u00e9 importante para redes neurais porque diferentes escalas de features podem levar a gradientes desbalanceados durante o treinamento, dificultando a converg\u00eancia do modelo. <code>MinMaxScaler</code> \u00e9 escolhido para manter a rela\u00e7\u00e3o original entre os valores, o que \u00e9 adequado para dados com distribui\u00e7\u00f5es variadas.</li> </ul> </li> <li> <p>Visualize os resultados:</p> <ul> <li>Crie histogramas para um ou dois recursos num\u00e9ricos (como FoodCourtou Age) antes e depois do dimensionamento para mostrar o efeito da sua transforma\u00e7\u00e3o.</li> </ul> </li> </ol> <p></p> <p>Obs: Partes desse relat\u00f3rio foram gerados com o aux\u00edlio de IA.</p>"},{"location":"mlp/main/","title":"Redes Neurais - Dados","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"mlp/main/#erik-soares","title":"Erik Soares","text":""},{"location":"mlp/main/#exercicio-1-calculo-manual-dos-passos-de-uma-mlp","title":"Exerc\u00edcio 1: C\u00e1lculo Manual dos Passos de uma MLP","text":"<p>Considere uma MLP simples com 2 features de entrada, 1 camada oculta contendo 2 neur\u00f4nios, e 1 neur\u00f4nio de sa\u00edda. Use a fun\u00e7\u00e3o de tangente hiperb\u00f3lica (tanh) como ativa\u00e7\u00e3o tanto para a camada oculta quanto para a camada de sa\u00edda. A fun\u00e7\u00e3o de perda \u00e9 o erro quadr\u00e1tico m\u00e9dio (MSE): \\(L = \\frac{1}{N} (y - \\hat{y})^2\\), onde \\(\\hat{y}\\) \u00e9 a sa\u00edda da rede.</p> <p>Para este exerc\u00edcio, use os seguintes valores espec\u00edficos:</p> <ul> <li>Vetores de entrada e sa\u00edda:     \\(\\mathbf{x} = [0.5, -0.2]\\) \\(y = 1.0\\)</li> <li>Pesos da camada oculta:     \\(\\mathbf{W}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix}\\)</li> <li>Bias da camada oculta:     \\(\\mathbf{b}^{(1)} = [0.1, -0.2]\\)</li> <li>Pesos da camada de sa\u00edda:     \\(\\mathbf{W}^{(2)} = [0.5, -0.3]\\)</li> <li>Bias da camada de sa\u00edda:     \\(b^{(2)} = 0.2\\)</li> <li>Taxa de aprendizado: \\(\\eta = 0.3\\)</li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: \\(\\tanh\\)</li> </ul> <p>Execute os seguintes passos explicitamente, mostrando todas as deriva\u00e7\u00f5es matem\u00e1ticas e c\u00e1lculos com os valores fornecidos:</p> <ol> <li>Passo Forward (Forward Pass):<ul> <li>Calcule as pr\u00e9-ativa\u00e7\u00f5es da camada oculta: \\(\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\\).</li> <li>Aplique a tanh para obter as ativa\u00e7\u00f5es ocultas: \\(\\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)})\\).</li> <li>Calcule a pr\u00e9-ativa\u00e7\u00e3o da sa\u00edda: \\(u^{(2)} = \\mathbf{W}^{(2)} \\mathbf{h}^{(1)} + b^{(2)}\\).</li> <li>Calcule a sa\u00edda final: \\(\\hat{y} = \\tanh(u^{(2)})\\).</li> </ul> </li> <li>C\u00e1lculo da Perda:<ul> <li>Calcule a perda MSE:     \\(L = \\frac{1}{N} (y - \\hat{y})^2\\).</li> </ul> </li> <li> <p>Passo Backward (Backpropagation): Calcule os gradientes da perda em rela\u00e7\u00e3o a todos os pesos e Bias.     Comece com \\(\\displaystyle \\frac{\\partial L}{\\partial \\hat{y}}\\), depois calcule:</p> <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial u^{(2)}}\\) (usando a derivada da tanh: \\(\\displaystyle \\frac{d}{du} \\tanh(u) = 1 - \\tanh^2(u)\\)).</li> <li>Gradientes para a camada de sa\u00edda: \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}}\\), \\(\\displaystyle \\frac{\\partial L}{\\partial b^{(2)}}\\).</li> <li>Propague para a camada oculta: \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}}\\), \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}}\\).</li> <li>Gradientes para a camada oculta: \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}\\), \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}}\\).</li> </ul> <p>Mostre todos os passos e c\u00e1lculos intermedi\u00e1rios.</p> </li> <li> <p>Atualiza\u00e7\u00e3o dos Par\u00e2metros: Usando a taxa de aprendizado \\(\\eta = 0.3\\)</p> <ul> <li>\\(\\displaystyle \\mathbf{W}^{(2)} \\leftarrow \\mathbf{W}^{(2)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}}\\)</li> <li>\\(\\displaystyle b^{(2)} \\leftarrow b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}}\\)</li> <li>\\(\\displaystyle \\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}\\)</li> <li>\\(\\displaystyle \\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}}\\)</li> </ul> <p>Forne\u00e7a os valores num\u00e9ricos para todos os par\u00e2metros atualizados.</p> </li> </ol> <p>Mostrando todos os passos matem\u00e1ticos explicitamente, incluindo c\u00e1lculos intermedi\u00e1rios (ex: multiplica\u00e7\u00f5es de matrizes, aplica\u00e7\u00f5es da tanh, deriva\u00e7\u00f5es de gradientes). Use valores num\u00e9ricos exatos e evite arredondar excessivamente para manter a precis\u00e3o (pelo menos 4 casas decimais).</p>"},{"location":"mlp/main/#solucao","title":"Solu\u00e7\u00e3o","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nnp.random.seed(42)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n# Vetores de entrada e sa\u00edda\nx = np.array([0.5, -0.2])\ny = 1\n\n# Pesos da camada oculta\nW1 = np.array([[0.3, -0.1], [0.2, 0.4]])\n\n# Bias da camada oculta\nb1 = np.array([0.1, -0.2])\n\n# Pesos da camada de sa\u00edda\nW2 = np.array([0.5, -0.3])\n\n# Bias da camada de sa\u00edda\nb2 = 0.2\n\n# Taxa de aprendizado\nlearning_rate = 0.3\n\n# Passo Forward\n# Pr\u00e9-ativa\u00e7\u00f5es da camada oculta\nz1 = np.dot(W1, x) + b1\n# Ativa\u00e7\u00f5es ocultas\nh1 = tanh(z1)\n# Pr\u00e9-ativa\u00e7\u00e3o da sa\u00edda\nu2 = np.dot(W2, h1) + b2\n# Sa\u00edda final\ny_ = tanh(u2)\n\n# C\u00e1lculo da perda\nL = (y_ - y)**2\n\nprint(f\"Valor Real (Alvo): {y}\")\nprint(f\"Previs\u00e3o da Rede: {y_:.4f}\")\nprint(f\"Erro Quadr\u00e1tico (Perda): {L:.4f}\\n\")\n\n# Passo Backward\n# Gradiente da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda\ndL_dy_ = 2 * (y_ - y)\nprint(f\"Gradiente da Perda em rela\u00e7\u00e3o \u00e0 Previs\u00e3o (dL/dy_): {dL_dy_:.4f}\")\n# Derivada da tanh na sa\u00edda\ndy_du2 = tanh_derivative(u2)\nprint(f\"Derivada da Ativa\u00e7\u00e3o de Sa\u00edda (dy_/du2): {dy_du2:.4f}\")\n# Sinal de erro na sa\u00edda\ndelta2 = dL_dy_ * dy_du2\nprint(f\"Sinal de Erro da Camada de Sa\u00edda (delta2): {delta2:.4f}\")\n# Gradientes para a camada de sa\u00edda\ndL_db2 = delta2\ndL_dW2 = delta2 * h1\nprint(f\"Gradiente do Bias de Sa\u00edda (dL/db2): {dL_db2:.4f}\")\nprint(f\"Gradiente dos Pesos de Sa\u00edda (dL/dW2): {dL_dW2}\\n\")\n\n# Camada oculta\n# Derivada de u2 em rela\u00e7\u00e3o a h1\ndu2_dh1 = W2\nprint(f\"Derivada da Pr\u00e9-ativa\u00e7\u00e3o de Sa\u00edda em rela\u00e7\u00e3o \u00e0 Ativa\u00e7\u00e3o Oculta (du2/dh1): {du2_dh1}\")\n# Propaga o sinal de erro para a sa\u00edda da camada oculta\ndL_dh1 = delta2 * du2_dh1\nprint(f\"Gradiente da Perda em rela\u00e7\u00e3o \u00e0 Ativa\u00e7\u00e3o Oculta (dL/dh1): {dL_dh1}\")\n# Derivada da tanh na camada oculta\ndh1_dz1 = tanh_derivative(z1)\nprint(f\"Derivada da Ativa\u00e7\u00e3o Oculta (dh1/dz1): {dh1_dz1}\")\n# Sinal de erro na camada oculta\ndelta1 = dL_dh1 * dh1_dz1\nprint(f\"Sinal de Erro da Camada Oculta (delta1): {delta1}\")\n# Gradientes para a camada oculta\ndL_db1 = delta1\ndL_dW1 = np.outer(delta1, x)\nprint(f\"Gradiente do Bias da Camada Oculta (dL/db1): {dL_db1}\")\nprint(f\"Gradiente dos Pesos da Camada Oculta (dL/dW1):\\n{dL_dW1}\\n\")\n\n# Atualiza\u00e7\u00e3o dos par\u00e2metros (pesos e Bias)\nW1 = W1 - learning_rate * dL_dW1\nb1 = b1 - learning_rate * dL_db1\n\nW2 = W2 - learning_rate * dL_dW2\nb2 = b2 - learning_rate * dL_db2\n\nprint(\"\\nPar\u00e2metros atualizados:\")\nprint(f\"Pesos da Camada Oculta (W1) Atualizados:\\n{W1}\")\nprint(f\"Bias da Camada Oculta (b1) Atualizado:\\n{b1}\")\nprint(f\"Pesos da Camada de Sa\u00edda (W2) Atualizados:\\n{W2}\")\nprint(f\"Bias da Camada de Sa\u00edda (b2) Atualizado:\\n{b2}\")\n</code></pre> <pre><code>Valor Real (Alvo): 1\nPrevis\u00e3o da Rede: 0.3672\nErro Quadr\u00e1tico (Perda): 0.4004\n\nGradiente da Perda em rela\u00e7\u00e3o \u00e0 Previs\u00e3o (dL/dy_): -1.2655\nDerivada da Ativa\u00e7\u00e3o de Sa\u00edda (dy_/du2): 0.8651\nSinal de Erro da Camada de Sa\u00edda (delta2): -1.0948\nGradiente do Bias de Sa\u00edda (dL/db2): -1.0948\nGradiente dos Pesos de Sa\u00edda (dL/dW2): [-0.28862383  0.19496791]\n\nDerivada da Pr\u00e9-ativa\u00e7\u00e3o de Sa\u00edda em rela\u00e7\u00e3o \u00e0 Ativa\u00e7\u00e3o Oculta (du2/dh1): [ 0.5 -0.3]\nGradiente da Perda em rela\u00e7\u00e3o \u00e0 Ativa\u00e7\u00e3o Oculta (dL/dh1): [-0.54741396  0.32844837]\nDerivada da Ativa\u00e7\u00e3o Oculta (dh1/dz1): [0.93050195 0.9682872 ]\nSinal de Erro da Camada Oculta (delta1): [-0.50936975  0.31803236]\nGradiente do Bias da Camada Oculta (dL/db1): [-0.50936975  0.31803236]\nGradiente dos Pesos da Camada Oculta (dL/dW1):\n[[-0.25468488  0.10187395]\n [ 0.15901618 -0.06360647]]\n\n\nPar\u00e2metros atualizados:\nPesos da Camada Oculta (W1) Atualizados:\n[[ 0.37640546 -0.13056219]\n [ 0.15229515  0.41908194]]\nBias da Camada Oculta (b1) Atualizado:\n[ 0.25281093 -0.29540971]\nPesos da Camada de Sa\u00edda (W2) Atualizados:\n[ 0.58658715 -0.35849037]\nBias da Camada de Sa\u00edda (b2) Atualizado:\n0.5284483744140799\n</code></pre>"},{"location":"mlp/main/#exercicio-2-classificacao-binaria-com-dados-sinteticos-e-mlp-do-zero","title":"Exerc\u00edcio 2: Classifica\u00e7\u00e3o Bin\u00e1ria com Dados Sint\u00e9ticos e MLP do Zero","text":"<p>Usando a fun\u00e7\u00e3o <code>make_classification</code> do scikit-learn (documenta\u00e7\u00e3o), gere um conjunto de dados sint\u00e9ticos com as seguintes especifica\u00e7\u00f5es:</p> <ul> <li>N\u00famero de amostras: 1000</li> <li>N\u00famero de classes: 2</li> <li>N\u00famero de clusters por classe: use o par\u00e2metro <code>n_clusters_per_class</code> de forma criativa para obter 1 cluster para uma classe e 2 para a outra (dica: pode ser necess\u00e1rio gerar subconjuntos separadamente e combin\u00e1-los, pois a fun\u00e7\u00e3o aplica o mesmo n\u00famero de clusters a todas as classes por padr\u00e3o).</li> <li>Outros par\u00e2metros: Defina <code>n_features=2</code> para f\u00e1cil visualiza\u00e7\u00e3o, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> para reprodutibilidade e ajuste <code>class_sep</code> ou <code>flip_y</code> conforme necess\u00e1rio para um conjunto de dados desafiador, mas separ\u00e1vel.</li> </ul> <p>Implemente um MLP do zero (sem usar bibliotecas como TensorFlow ou PyTorch para o modelo em si; voc\u00ea pode usar NumPy para opera\u00e7\u00f5es de array) para classificar esses dados. Voc\u00ea tem total liberdade para escolher a arquitetura, incluindo:</p> <ul> <li>N\u00famero de camadas ocultas (pelo menos 1)</li> <li>N\u00famero de neur\u00f4nios por camada</li> <li>Fun\u00e7\u00f5es de ativa\u00e7\u00e3o (por exemplo, sigm\u00f3ide, ReLU, tanh)</li> <li>Fun\u00e7\u00e3o de perda (por exemplo, entropia cruzada bin\u00e1ria)</li> <li>Otimizador (por exemplo, descida de gradiente, com uma taxa de aprendizagem escolhida)</li> </ul>"},{"location":"mlp/main/#passos-a-seguir","title":"Passos a seguir:","text":"<ol> <li>Gere e divida os dados em conjuntos de treinamento (80%) e de teste (20%).</li> <li>Implemente a passagem para frente, o c\u00e1lculo de perdas, a passagem para tr\u00e1s e as atualiza\u00e7\u00f5es de par\u00e2metros no c\u00f3digo.</li> <li>Treine o modelo para um n\u00famero razo\u00e1vel de \u00e9pocas (por exemplo, 100-500), rastreando a perda de treinamento.</li> <li>Avalie no conjunto de teste: relate a precis\u00e3o e, opcionalmente, trace limites de decis\u00e3o ou matriz de confus\u00e3o.</li> <li>Envie seu c\u00f3digo e resultados, incluindo quaisquer visualiza\u00e7\u00f5es.</li> </ol>"},{"location":"mlp/main/#solucao_1","title":"Solu\u00e7\u00e3o","text":""},{"location":"mlp/main/#1-geracao-e-divisao-dos-dados","title":"1. Gera\u00e7\u00e3o e Divis\u00e3o dos Dados","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\n\n\n#  Gera dados apenas para a Classe 0 com 1 cluster\n# Para isso, geramos 2 classes com 1 cluster cada e pegamos apenas a primeira\nX_temp1, y_temp1 = make_classification(\n    n_samples=1000, # Geramos mais para garantir pelo menos 500 amostras da classe 0\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=2,\n    n_clusters_per_class=1,\n    class_sep=1.5,\n    random_state=42\n)\n# Filtramos e pegamos as primeiras 500 amostras da classe 0\nX0 = X_temp1[y_temp1 == 0][:500]\ny0 = np.zeros((500, 1))\n\n\n# Gera dados apenas para a Classe 1 com 2 clusters\nX_temp2, y_temp2 = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=2,\n    n_clusters_per_class=2,\n    class_sep=1.5,\n    random_state=43\n)\n# Filtramos e pegamos as primeiras 500 amostras da classe 1\nX1 = X_temp2[y_temp2 == 1][:500]\ny1 = np.ones((500, 1))\n\nX = np.vstack((X0, X1))\ny = np.vstack((y0, y1))\n\n# Embaralhando o dataset\nshuffle_idx = np.random.permutation(len(X))\nX = X[shuffle_idx]\ny = y[shuffle_idx]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\nplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='coolwarm', edgecolors='k')\nplt.title(\"Dataset divis\u00e3o das classes\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n</code></pre>"},{"location":"mlp/main/#2-implementacao-do-mlp-do-zero","title":"2. Implementa\u00e7\u00e3o do MLP do Zero","text":""},{"location":"mlp/main/#funcoes-auxiliares","title":"Fun\u00e7\u00f5es Auxiliares","text":"<pre><code>def sigmoid(Z):\n    \"\"\"Calcula a fun\u00e7\u00e3o sigmoide\"\"\"\n    return 1 / (1 + np.exp(-Z))\n\ndef relu(Z):\n    \"\"\"Calcula a fun\u00e7\u00e3o ReLU.\"\"\"\n    return np.maximum(0, Z)\n\n\ndef relu_derivative(Z):\n    \"\"\"Calcula a derivada da ReLU\"\"\"\n    return np.where(Z &gt; 0, 1, 0)\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"Inicializa os pesos e biases da rede\"\"\"\n    np.random.seed(42)\n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros((n_y, 1))\n    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n    return parameters\n\ndef forward_pass(X, parameters):\n    \"\"\"Executa a passagem para frente (forward propagation)\"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n    Z1 = np.dot(W1, X.T) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n    return A2, cache\n\ndef compute_loss(A2, Y):\n    \"\"\"Calcula a perda (Entropia cruzda binaria)\"\"\"\n    m = Y.shape[0]\n    epsilon = 1e-8\n    loss = -1/m * np.sum(Y.T * np.log(A2 + epsilon) + (1 - Y.T) * np.log(1 - A2 + epsilon))\n    return np.squeeze(loss)\n\ndef backward_pass(parameters, cache, X, Y):\n    \"\"\"Executa o backpropagation\"\"\"\n    m = X.shape[0]\n    W2 = parameters[\"W2\"]\n    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n    Z1 = cache[\"Z1\"]\n\n    dZ2 = A2 - Y.T\n    dW2 = 1/m * np.dot(dZ2, A1.T)\n    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * relu_derivative(Z1)\n    dW1 = 1/m * np.dot(dZ1, X)\n    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n\n    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"Atualiza os pesos e biases usando gradiente descendente.\"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n\n    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n    return parameters\n\ndef predict(X, parameters):\n    \"\"\"Faz previs\u00f5es usando o modelo treinado.\"\"\"\n    A2, _ = forward_pass(X, parameters)\n    predictions = (A2 &gt; 0.5)\n    return predictions.astype(int)\n</code></pre>"},{"location":"mlp/main/#treinamento-do-modelo-e-plotagem-da-curva-de-aprendizado","title":"Treinamento do Modelo e Plotagem da Curva de Aprendizado","text":"<p><pre><code>def mlp_model(X, Y, n_h, num_epochs, learning_rate):\n    \"\"\"Constr\u00f3i e treina o modelo MLP completo.\"\"\"\n    n_x = X.shape[1]\n    n_y = Y.shape[1]\n\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    losses = []\n\n    for i in range(num_epochs):\n        A2, cache = forward_pass(X, parameters)\n        loss = compute_loss(A2, Y)\n        grads = backward_pass(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        if i % 100 == 0:\n            losses.append(loss)\n            print(f\"\u00c9poca {i}, Perda: {loss:.4f}\")\n\n    return parameters, losses\n\ntrained_parameters, training_losses = mlp_model(\n    X_train, y_train, n_h=2, num_epochs=1000, learning_rate=0.1\n)\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(training_losses)\nplt.title(\"Curva de Aprendizado (Perda de Treinamento)\")\nplt.xlabel(\"\u00c9pocas (x100)\")\nplt.ylabel(\"Perda (Entropia Cruzada Bin\u00e1ria)\")\nplt.grid(True, linestyle='--')\nplt.show()\n</code></pre> </p>"},{"location":"mlp/main/#3-avaliacao-do-modelo-no-conjunto-de-teste","title":"3. Avalia\u00e7\u00e3o do Modelo no Conjunto de Teste","text":"<p><pre><code>test_predictions = predict(X_test, trained_parameters)\naccuracy = np.mean(test_predictions == y_test.T) * 100\nprint(f\"\\nAcur\u00e1cia no conjunto de teste: {accuracy:.2f}%\")\n\n\ndef plot_decision_boundary(X, y, parameters):\n    \"\"\"Plota a fronteira de decis\u00e3o aprendida pelo modelo.\"\"\"\n    plt.figure(figsize=(10, 7))\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    h = 0.01\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    Z = predict(np.c_[xx.ravel(), yy.ravel()], parameters)\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.6)\n    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='coolwarm', edgecolors='k', s=50)\n\n    plt.title(\"Fronteira de Decis\u00e3o no Conjunto de Teste\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()\n\n# Visualizando a fronteira de decis\u00e3o com os dados de teste\nplot_decision_boundary(X_test, y_test, trained_parameters)\n</code></pre> - Acur\u00e1cia no conjunto de teste: 82.00% </p>"},{"location":"mlp/main/#exercicio-3-classificacao-multiclasse-com-dados-sinteticos-e-mlp-reutilizavel","title":"Exerc\u00edcio 3: Classifica\u00e7\u00e3o Multiclasse com Dados Sint\u00e9ticos e MLP Reutiliz\u00e1vel","text":"<p>Semelhante ao Exerc\u00edcio 2, mas com maior complexidade. Use <code>make_classification</code> para gerar um conjunto de dados sint\u00e9ticos com:</p> <ul> <li>N\u00famero de amostras: 1500</li> <li>N\u00famero de classes: 3</li> <li>N\u00famero de features: 4</li> <li>N\u00famero de clusters por classe: obtenha 2 clusters para uma classe, 3 para outra e 4 para a \u00faltima (novamente, pode ser necess\u00e1rio gerar subconjuntos separadamente e combin\u00e1-los, pois a fun\u00e7\u00e3o n\u00e3o oferece suporte direto \u00e0 varia\u00e7\u00e3o de clusters por classe).</li> <li>Outros par\u00e2metros: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implemente um MLP do zero para classificar esses dados. Voc\u00ea pode escolher a arquitetura livremente, mas, para ganhar um ponto extra (elevando este exerc\u00edcio para 4 pontos), reutilize exatamente o mesmo c\u00f3digo de implementa\u00e7\u00e3o do MLP do Exerc\u00edcio 2, modificando apenas os hiperpar\u00e2metros (por exemplo, tamanho da camada de sa\u00edda para 3 classes, fun\u00e7\u00e3o de perda para entropia cruzada categ\u00f3rica, se necess\u00e1rio) sem alterar a estrutura principal.</p> <p>Passos:</p> <ol> <li>Gerar e dividir os dados (treinamento/teste 80/20).</li> <li>Treine o modelo, rastreando perdas.</li> <li>Avalie no conjunto de teste: relate a precis\u00e3o e, opcionalmente, visualize (por exemplo, gr\u00e1fico de dispers\u00e3o de dados com r\u00f3tulos previstos).</li> <li>Envie o c\u00f3digo e os resultados.</li> </ol>"},{"location":"mlp/main/#solucao_2","title":"Solu\u00e7\u00e3o","text":""},{"location":"mlp/main/#1-geracao-e-divisao-dos-dados_1","title":"1. Gera\u00e7\u00e3o e Divis\u00e3o dos Dados","text":"<p><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.decomposition import PCA\n\n# Gera dados para a Classe 0 com 2 clusters\nX_temp0, y_temp0 = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_classes=2,\n    n_clusters_per_class=2,\n    class_sep=1.5,\n    random_state=42\n)\nX0 = X_temp0[y_temp0 == 0][:500]\ny0 = np.zeros((500, 1))\n\n# Gera dados para a Classe 1 com 3 clusters\nX_temp1, y_temp1 = make_classification(\n    n_samples=1500,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_classes=2,\n    n_clusters_per_class=3,\n    class_sep=1.5,\n    random_state=43\n)\nX1 = X_temp1[y_temp1 == 1][:500]\ny1 = np.ones((500, 1))\n\n# Gera dados para a Classe 2 com 4 clusters\nX_temp2, y_temp2 = make_classification(\n    n_samples=2000,\n    n_features=4,\n    n_informative=4,\n    n_redundant=0,\n    n_classes=2,\n    n_clusters_per_class=4,\n    class_sep=1.5,\n    random_state=44\n)\nX2 = X_temp2[y_temp2 == 0][:500]\ny2 = np.full((500, 1), 2)\n\nX = np.vstack((X0, X1, X2))\ny = np.vstack((y0, y1, y2))\n\nshuffle_idx = np.random.permutation(len(X))\nX = X[shuffle_idx]\ny = y[shuffle_idx]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\npca = PCA(n_components=2)\npca.fit(X_train)\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nplt.figure(figsize=(10, 8)) \n\nclasses = np.unique(y_train)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n\nfor i, class_label in enumerate(classes):\n    # Filtra os dados para pegar apenas os pontos da classe atual\n    X_class = X_train_pca[y_train.ravel() == class_label]\n    # Plota os pontos da classe atual com sua cor e r\u00f3tulo espec\u00edficos\n    plt.scatter(X_class[:, 0], X_class[:, 1],\n                color=colors[i],\n                label=f'Classe {int(class_label)}',\n                edgecolors='k', alpha=0.8)\n\n\nplt.title(\"Dataset Multiclasse (Visualiza\u00e7\u00e3o com PCA)\")\nplt.xlabel(\"Componente Principal 1\")\nplt.ylabel(\"Componente Principal 2\")\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.legend()\nplt.show()\n</code></pre> </p>"},{"location":"mlp/main/#2-implementacao-do-mlp-do-zero_1","title":"2. Implementa\u00e7\u00e3o do MLP do Zero","text":""},{"location":"mlp/main/#funcoes-auxiliares_1","title":"Fun\u00e7\u00f5es Auxiliares","text":"<pre><code>def softmax(Z):\n    \"\"\"Calcula a fun\u00e7\u00e3o softmax.\"\"\"\n    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n    return expZ / np.sum(expZ, axis=0, keepdims=True)\n\ndef relu(Z):\n    \"\"\"Calcula a fun\u00e7\u00e3o ReLU.\"\"\"\n    return np.maximum(0, Z)\n\ndef relu_derivative(Z):\n    \"\"\"Calcula a derivada da ReLU\"\"\"\n    return np.where(Z &gt; 0, 1, 0)\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"Inicializa os pesos e biases da rede\"\"\"\n    np.random.seed(42)\n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros((n_y, 1))\n    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n    return parameters\n\ndef forward_pass(X, parameters):\n    \"\"\"Executa a passagem para frente (forward propagation)\"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n    Z1 = np.dot(W1, X.T) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = softmax(Z2)\n    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n    return A2, cache\n\ndef compute_loss(A2, Y_one_hot):\n    \"\"\"Calcula a perda (Entropia cruzada categ\u00f3rica).\"\"\"\n    m = Y_one_hot.shape[1]\n    epsilon = 1e-8\n    loss = -1/m * np.sum(Y_one_hot * np.log(A2 + epsilon))\n    return np.squeeze(loss)\n\ndef backward_pass(parameters, cache, X, Y_one_hot):\n    \"\"\"Executa o backpropagation\"\"\"\n    m = X.shape[0]\n    W2 = parameters[\"W2\"]\n    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n    Z1 = cache[\"Z1\"]\n\n    dZ2 = A2 - Y_one_hot\n    dW2 = 1/m * np.dot(dZ2, A1.T)\n    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * relu_derivative(Z1)\n    dW1 = 1/m * np.dot(dZ1, X)\n    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n\n    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"Atualiza os pesos e biases usando gradiente descendente.\"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n\n    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n    return parameters\n\ndef predict(X, parameters):\n    \"\"\"Faz previs\u00f5es usando o modelo treinado.\"\"\"\n    A2, _ = forward_pass(X, parameters)\n    # A previs\u00e3o \u00e9 o \u00edndice da classe com maior probabilidade\n    predictions = np.argmax(A2, axis=0)\n    return predictions\n\ndef one_hot(y, num_classes):\n    \"\"\"Converte um vetor de r\u00f3tulos para o formato one-hot.\"\"\"\n    y = y.astype(int)\n    y_one_hot = np.eye(num_classes)[y.ravel()]\n    return y_one_hot.T\n</code></pre>"},{"location":"mlp/main/#treinamento-do-modelo-e-plotagem-da-curva-de-aprendizado_1","title":"Treinamento do Modelo e Plotagem da Curva de Aprendizado","text":"<p><pre><code>def mlp_model(X, Y, n_h, num_epochs, learning_rate):\n    \"\"\"Constr\u00f3i e treina o modelo MLP completo.\"\"\"\n    n_x = X.shape[1]\n    num_classes = len(np.unique(Y))\n    n_y = num_classes\n\n    Y_one_hot = one_hot(Y, num_classes)\n\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    losses = []\n\n    for i in range(num_epochs):\n        A2, cache = forward_pass(X, parameters)\n        loss = compute_loss(A2, Y_one_hot)\n        grads = backward_pass(parameters, cache, X, Y_one_hot)\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        if i % 100 == 0:\n            losses.append(loss)\n            print(f\"\u00c9poca {i}, Perda: {loss:.4f}\")\n\n    return parameters, losses\n\ntrained_parameters, training_losses = mlp_model(\n    X_train, y_train, n_h=2, num_epochs=1000, learning_rate=0.1\n)\n\nplt.figure(figsize=(8, 6))\nplt.plot(training_losses)\nplt.title(\"Curva de Aprendizado (Perda de Treinamento)\")\nplt.xlabel(\"\u00c9pocas (x100)\")\nplt.ylabel(\"Perda (Categorical Cross-Entropy)\")\nplt.grid(True, linestyle='--')\nplt.show()\n</code></pre> </p>"},{"location":"mlp/main/#3-avaliacao-do-modelo-no-conjunto-de-teste_1","title":"3. Avalia\u00e7\u00e3o do Modelo no Conjunto de Teste","text":"<p>Como temos 4 dimens\u00f5es nos dados, utilizamos PCA para ajudar na visualiza\u00e7\u00e3o <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA \n\ntest_predictions = predict(X_test, trained_parameters)\n\naccuracy = np.mean(test_predictions == y_test.ravel()) * 100\nprint(f\"\\nAcur\u00e1cia no conjunto de teste: {accuracy:.2f}%\")\n\npca = PCA(n_components=2)\n\n# Treinamos o PCA apenas com os dados de treino\npca.fit(X_train)\n\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\ndef plot_decision_boundary_pca(X_pca, y_data, pca_object, parameters):\n    \"\"\"Plota a fronteira de decis\u00e3o no espa\u00e7o PCA.\"\"\"\n    plt.figure(figsize=(10, 7))\n\n    x_min, x_max = X_pca[:, 0].min() - 0.5, X_pca[:, 0].max() + 0.5\n    y_min, y_max = X_pca[:, 1].min() - 0.5, X_pca[:, 1].max() + 0.5\n\n    h = 0.02\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    grid_points_2d = np.c_[xx.ravel(), yy.ravel()]\n\n    # Usa a transforma\u00e7\u00e3o inversa do PCA para converter os pontos da malha\n    # de volta para o espa\u00e7o 4D original, para que o modelo possa prever\n    grid_points_4d_original = pca_object.inverse_transform(grid_points_2d)\n\n    Z = predict(grid_points_4d_original, parameters)\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.6)\n    # Plota os dados de teste j\u00e1 transformados pelo PCA\n    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_data.ravel(), cmap='viridis', edgecolors='k', s=50)\n\n    plt.title(\"Fronteira de Decis\u00e3o no Espa\u00e7o PCA\")\n    plt.xlabel(\"Componente Principal 1\")\n    plt.ylabel(\"Componente Principal 2\")\n    plt.show()\n\nplot_decision_boundary_pca(X_test_pca, y_test, pca, trained_parameters)\n</code></pre> - Acur\u00e1cia no conjunto de teste: 66.67% </p>"},{"location":"mlp/main/#exercicio-4-classificacao-multiclasse-com-mlp-mais-profundo","title":"Exerc\u00edcio 4: Classifica\u00e7\u00e3o multiclasse com MLP mais profundo","text":"<p>Repita o Exerc\u00edcio 3 exatamente, mas agora certifique-se de que seu MLP tenha pelo menos 2 camadas ocultas. Voc\u00ea pode ajustar o n\u00famero de neur\u00f4nios por camada conforme necess\u00e1rio para obter melhor desempenho. Reutilize o c\u00f3digo do Exerc\u00edcio 3 sempre que poss\u00edvel, mas o foco est\u00e1 em demonstrar a arquitetura mais profunda. Envie o c\u00f3digo atualizado, os resultados do treinamento e a avalia\u00e7\u00e3o dos testes.</p>"},{"location":"mlp/main/#solucao_3","title":"Solu\u00e7\u00e3o","text":""},{"location":"mlp/main/#1-treinamento-do-modelo-com-mlp-mais-profundo","title":"1. Treinamento do Modelo com MLP mais Profundo","text":"<p><pre><code>def mlp_model(X, Y, n_h, num_epochs, learning_rate):\n    \"\"\"Constr\u00f3i e treina o modelo MLP completo.\"\"\"\n    n_x = X.shape[1]\n    num_classes = len(np.unique(Y))\n    n_y = num_classes\n\n    Y_one_hot = one_hot(Y, num_classes)\n\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    losses = []\n\n    for i in range(num_epochs):\n        A2, cache = forward_pass(X, parameters)\n        loss = compute_loss(A2, Y_one_hot)\n        grads = backward_pass(parameters, cache, X, Y_one_hot)\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        if i % 100 == 0:\n            losses.append(loss)\n            print(f\"\u00c9poca {i}, Perda: {loss:.4f}\")\n\n    return parameters, losses\n\ntrained_parameters, training_losses = mlp_model(\n    X_train, y_train, n_h=20, num_epochs=1000, learning_rate=0.1\n)\n\nplt.figure(figsize=(8, 6))\nplt.plot(training_losses)\nplt.title(\"Curva de Aprendizado (Perda de Treinamento)\")\nplt.xlabel(\"\u00c9pocas (x100)\")\nplt.ylabel(\"Perda (Categorical Cross-Entropy)\")\nplt.grid(True, linestyle='--')\nplt.show()\n</code></pre> </p>"},{"location":"mlp/main/#2-avaliacao-do-modelo-no-conjunto-de-teste","title":"2. Avalia\u00e7\u00e3o do Modelo no Conjunto de Teste","text":"<p><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA \n\ntest_predictions = predict(X_test, trained_parameters)\n\naccuracy = np.mean(test_predictions == y_test.ravel()) * 100\nprint(f\"\\nAcur\u00e1cia no conjunto de teste: {accuracy:.2f}%\")\n\npca = PCA(n_components=2)\n\n# Treinamos o PCA apenas com os dados de treino\npca.fit(X_train)\n\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\ndef plot_decision_boundary_pca(X_pca, y_data, pca_object, parameters):\n    \"\"\"Plota a fronteira de decis\u00e3o no espa\u00e7o PCA.\"\"\"\n    plt.figure(figsize=(10, 7))\n\n    x_min, x_max = X_pca[:, 0].min() - 0.5, X_pca[:, 0].max() + 0.5\n    y_min, y_max = X_pca[:, 1].min() - 0.5, X_pca[:, 1].max() + 0.5\n\n    h = 0.02\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    grid_points_2d = np.c_[xx.ravel(), yy.ravel()]\n\n    # Usa a transforma\u00e7\u00e3o inversa do PCA para converter os pontos da malha\n    # de volta para o espa\u00e7o 4D original, para que o modelo possa prever\n    grid_points_4d_original = pca_object.inverse_transform(grid_points_2d)\n\n    Z = predict(grid_points_4d_original, parameters)\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.6)\n    # Plota os dados de teste j\u00e1 transformados pelo PCA\n    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_data.ravel(), cmap='viridis', edgecolors='k', s=50)\n\n    plt.title(\"Fronteira de Decis\u00e3o no Espa\u00e7o PCA\")\n    plt.xlabel(\"Componente Principal 1\")\n    plt.ylabel(\"Componente Principal 2\")\n    plt.show()\n\nplot_decision_boundary_pca(X_test_pca, y_test, pca, trained_parameters)\n</code></pre> - Acur\u00e1cia no conjunto de teste: 81.33% </p> <p>Obs: Partes desse relat\u00f3rio foram gerados com o aux\u00edlio de IA.</p>"},{"location":"perceptron/main/","title":"Redes Neurais - Perceptron","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"perceptron/main/#erik-soares","title":"Erik Soares","text":"<p>Esta atividade tem como objetivo a implementa\u00e7\u00e3o e testes com Perceptrons e suas limita\u00e7\u00f5es.</p>"},{"location":"perceptron/main/#parte-1-dados-linearmente-separaveis","title":"Parte 1 - Dados Linearmente Separ\u00e1veis","text":""},{"location":"perceptron/main/#geracao-de-dados","title":"Gera\u00e7\u00e3o de Dados","text":"<ol> <li> <p>Gera\u00e7\u00e3o dos Dados: Gera\u00e7\u00e3o de duas classes de pontos de dados 2D (1.000 amostras por classe) usando distribui\u00e7\u00f5es normais multivariadas. Com base nos seguintes par\u00e2metros:</p> <ul> <li>Classe 0: M\u00e9dia = [1.5, 1.5], Desvio Padr\u00e3o = [[0.5, 0],[0, 0.5]]</li> <li>Classe 1: M\u00e9dia = [5, 5], Desvio Padr\u00e3o = [[0.5, 0],[0, 0.5]]</li> </ul> </li> <li> <p>Plotagem dos Dados: Esses par\u00e2metros garantem que as classes sejam separ\u00e1veis \u200b\u200blinearmente, com sobreposi\u00e7\u00e3o m\u00ednima devido \u00e0 dist\u00e2ncia entre m\u00e9dias e \u00e0 baixa vari\u00e2ncia.</p> </li> </ol>"},{"location":"perceptron/main/#geracao-e-plotagem-dos-dados","title":"Gera\u00e7\u00e3o e Plotagem dos Dados","text":"<p>O c\u00f3digo abaixo gera os dados sint\u00e9ticos conforme as especifica\u00e7\u00f5es e os plota em um gr\u00e1fico de dispers\u00e3o 2D. <pre><code>n_amostras_0 = 1000\nn_amostras_1 = 1000\n\nmedia_0 = np.array([1.5, 1.5])\nmedia_1 = np.array([5, 5])\n\ncovariancia_0 = np.array([\n    [0.5, 0],\n    [0, 0.5]\n])\n\ncovariancia_1 = np.array([\n    [0.5, 0],\n    [0, 0.5]\n])\n\ndados_0 = np.random.multivariate_normal(mean=media_0, cov=covariancia_0, size=n_amostras_0)\ndados_1 = np.random.multivariate_normal(mean=media_1, cov=covariancia_1, size=n_amostras_1)\ndados = np.concatenate([dados_0, dados_1])\nrotulos_0 = np.zeros(n_amostras_0)\nrotulos_1 = np.ones(n_amostras_1)\nrotulos = np.concatenate([rotulos_0, rotulos_1])\ndados_com_rotulos = np.c_[dados, rotulos]\ndf = pd.DataFrame(dados_com_rotulos, columns=['x1', 'x2', 'y'])\ndf = df.sample(frac=1)\n\nplt.figure(figsize=(8, 8))\n\nplt.scatter(dados_0[:, 0], dados_0[:, 1], c='blue', label='Classe 0', alpha=0.7)\n\nplt.scatter(dados_1[:, 0], dados_1[:, 1], c='red', label='Classe 1', alpha=0.7)\n\nplt.title('Dados Gerados para as Duas Classes')\nplt.xlabel('Caracter\u00edstica 1 (X1)')\nplt.ylabel('Caracter\u00edstica 2 (X2)')\nplt.legend()\nplt.grid(True)\nplt.axis('equal') \nplt.show()\n</code></pre> </p> <ol> <li>Implementa\u00e7\u00e3o de Perceptron:  Implementa\u00e7\u00e3o de um perceptron de camada \u00fanica do zero para classificar os dados gerados em duas classes.</li> <li> <p>Inicializa\u00e7\u00e3o dos pesos (w) como um vetor 2D (mais um termo de polariza\u00e7\u00e3o b).</p> </li> <li> <p>Uso da regra de aprendizado do perceptron: para cada amostra classificada incorretamente \\((x, y)\\), atualiza\u00e7\u00e3o de \\(w = w + \\eta \\cdot y \\cdot x\\) e \\(b = b + \\eta \\cdot y\\), onde \\(\\eta\\) \u00e9 a taxa de aprendizagem (come\u00e7ando com \\(\\eta = 0.01\\)).</p> </li> <li> <p>Treinamento do modelo at\u00e9 a converg\u00eancia ou por um m\u00e1ximo de 100 \u00e9pocas, o que ocorrer primeiro.</p> </li> </ol>"},{"location":"perceptron/main/#perceptron","title":"Perceptron","text":"<p>O c\u00f3digo abaixo implementa o perceptron e realiza o treinamento nos dados gerados. <pre><code>def perceptron(X, y, epocas=100, b=0, eta=0.01):\n    w = np.array([0.0, 0.0])\n    acuracias = []\n\n    for _ in tqdm(range(epocas)):\n        erros = 0\n        for i in range(len(X)):\n            z = np.dot(X[i], w) + b\n            y_pred = 1.0 if z &gt;= 0.0 else 0.0\n            erro = y[i] - y_pred\n            if erro != 0:\n                erros += 1\n                w = w + eta * erro * X[i]\n                b = b + eta * erro\n\n        acuracias.append((len(X) - erros)/len(X))\n        if erros == 0:\n            break\n\n    return w, b, acuracias\n\nX = df[[\"x1\", \"x2\"]].values\ny = df[\"y\"].values\n\nw1, b1, precisoes1 = perceptron(X, y)\n</code></pre></p>"},{"location":"perceptron/main/#visualizacao-dos-resultados","title":"Visualiza\u00e7\u00e3o dos Resultados","text":"<p>Os gr\u00e1ficos abaixo mostram a evolu\u00e7\u00e3o da acur\u00e1cia ao longo das \u00e9pocas e a fronteira de decis\u00e3o aprendida pelo perceptron. <pre><code>plt.figure(figsize=(10, 6))\nplt.plot(range(len(precisoes1)), precisoes1, marker='o', linestyle='-', color='b')\nplt.title('Evolu\u00e7\u00e3o da Acur\u00e1cia por \u00c9poca')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.grid(True)\nplt.show()\n</code></pre></p> <p></p> <p><pre><code>plt.figure(figsize=(8, 8))\n\nplt.scatter(dados_0[:, 0], dados_0[:, 1], c='blue', label='Classe 0', alpha=0.7)\nplt.scatter(dados_1[:, 0], dados_1[:, 1], c='red', label='Classe 1', alpha=0.7)\n\nx_min, x_max = plt.xlim()\nx1_plot = np.array([x_min, x_max])\nx2_plot = (-w1[0] * x1_plot - b1) / w1[1]\n\nplt.plot(x1_plot, x2_plot, 'k-', label='Fronteira de Decis\u00e3o')\n\nplt.title('Dados e Fronteira de Decis\u00e3o do Perceptron')\nplt.xlabel('Caracter\u00edstica 1 (X1)')\nplt.ylabel('Caracter\u00edstica 2 (X2)')\nplt.legend()\nplt.grid(True)\nplt.axis('equal')\n\nplt.show()\n</code></pre> </p>"},{"location":"perceptron/main/#resultados-finais","title":"Resultados Finais","text":"<p>Pesos Finais: [0.0527598, 0.04278312]</p> <p>Bias Final: -0.33</p> <p>Precis\u00e3o Final: 1.0</p> <p>Ao utilizarmos um conjunto de dados que \u00e9 linearmente separ\u00e1vel, o perceptron consegue encontrar uma fronteira de decis\u00e3o que separa perfeitamente as duas classes. Isso ocorre porque os dados foram gerados com m\u00e9dias distintas e baixa vari\u00e2ncia, o que minimiza a sobreposi\u00e7\u00e3o entre as classes. Como resultado, o perceptron pode ajustar seus pesos e vi\u00e9s rapidamente para alcan\u00e7ar uma precis\u00e3o de 100% em poucas \u00e9pocas de treinamento, j\u00e1 que necessita de apenas algumas atualiza\u00e7\u00f5es para alinhar a fronteira de decis\u00e3o corretamente.</p>"},{"location":"perceptron/main/#parte-2-dados-nao-linearmente-separaveis","title":"Parte 2 - Dados N\u00e3o Linearmente Separ\u00e1veis","text":""},{"location":"perceptron/main/#geracao-de-dados_1","title":"Gera\u00e7\u00e3o de Dados","text":"<ol> <li> <p>Gera\u00e7\u00e3o dos Dados: Gera\u00e7\u00e3o de duas classes de pontos de dados 2D (1.000 amostras por classe) usando distribui\u00e7\u00f5es normais multivariadas. Com base nos seguintes par\u00e2metros:</p> <ul> <li>Classe 0: M\u00e9dia = [3, 3], Desvio Padr\u00e3o = [[1.5, 0],[0, 1.5]]</li> <li>Classe 1: M\u00e9dia = [4, 4], Desvio Padr\u00e3o = [[1.5, 0],[0, 1.5]]</li> </ul> </li> <li> <p>Plotagem dos Dados: Esses par\u00e2metros criam sobreposi\u00e7\u00e3o parcial entre as classes devido a m\u00e9dias mais pr\u00f3ximas e maior vari\u00e2ncia, tornando os dados n\u00e3o totalmente separ\u00e1veis \u200b\u200blinearmente</p> </li> </ol>"},{"location":"perceptron/main/#geracao-e-plotagem-dos-dados_1","title":"Gera\u00e7\u00e3o e Plotagem dos Dados","text":"<p>O c\u00f3digo abaixo gera os dados sint\u00e9ticos conforme as especifica\u00e7\u00f5es e os plota em um gr\u00e1fico de dispers\u00e3o 2D. <pre><code>n_amostras_0 = 1000\nn_amostras_1 = 1000\n\nmedia_0 = np.array([3, 3])\nmedia_1 = np.array([4, 4])\n\ncovariancia_0 = np.array([\n    [1.5, 0],\n    [0, 1.5]\n])\n\ncovariancia_1 = np.array([\n    [1.5, 0],\n    [0, 1.5]\n])\n\ndados_0 = np.random.multivariate_normal(mean=media_0, cov=covariancia_0, size=n_amostras_0)\ndados_1 = np.random.multivariate_normal(mean=media_1, cov=covariancia_1, size=n_amostras_1)\ndados = np.concatenate([dados_0, dados_1])\nrotulos_0 = np.zeros(n_amostras_0)\nrotulos_1 = np.ones(n_amostras_1)\nrotulos = np.concatenate([rotulos_0, rotulos_1])\ndados_com_rotulos = np.c_[dados, rotulos]\ndf = pd.DataFrame(dados_com_rotulos, columns=['x1', 'x2', 'y'])\ndf = df.sample(frac=1)\n\nplt.figure(figsize=(8, 8))\n\nplt.scatter(dados_0[:, 0], dados_0[:, 1], c='blue', label='Classe 0', alpha=0.7)\n\nplt.scatter(dados_1[:, 0], dados_1[:, 1], c='red', label='Classe 1', alpha=0.7)\n\nplt.title('Dados Gerados para as Duas Classes')\nplt.xlabel('Caracter\u00edstica 1 (X1)')\nplt.ylabel('Caracter\u00edstica 2 (X2)')\nplt.legend()\nplt.grid(True)\nplt.axis('equal') \nplt.show()\n</code></pre> </p> <ol> <li>Implementa\u00e7\u00e3o de Perceptron:  Implementa\u00e7\u00e3o de um perceptron seguindo as mesmas etapas do exerc\u00edcio anterior.</li> </ol>"},{"location":"perceptron/main/#perceptron_1","title":"Perceptron","text":"<p>O c\u00f3digo abaixo implementa o perceptron e realiza o treinamento nos dados gerados, seguindo as mesmas etapas do exerc\u00edcio anterior. <pre><code>X = df[[\"x1\", \"x2\"]].values\ny = df[\"y\"].values\n\nw2, b2, precisoes2 = perceptron(X, y)\n</code></pre></p>"},{"location":"perceptron/main/#visualizacao-dos-resultados_1","title":"Visualiza\u00e7\u00e3o dos Resultados","text":"<p>Os gr\u00e1ficos abaixo mostram a evolu\u00e7\u00e3o da acur\u00e1cia ao longo das \u00e9pocas e a fronteira de decis\u00e3o aprendida pelo perceptron.</p> <p><pre><code>plt.figure(figsize=(10, 6))\nplt.plot(range(len(precisoes2)), precisoes2, marker='o', linestyle='-', color='b')\nplt.title('Evolu\u00e7\u00e3o da Acur\u00e1cia por \u00c9poca')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.grid(True)\nplt.show()\n</code></pre> </p> <p><pre><code>plt.figure(figsize=(8, 8))\n\nplt.scatter(dados_0[:, 0], dados_0[:, 1], c='blue', label='Classe 0', alpha=0.7)\nplt.scatter(dados_1[:, 0], dados_1[:, 1], c='red', label='Classe 1', alpha=0.7)\n\nx_min, x_max = plt.xlim()\nx1_plot = np.array([x_min, x_max])\nx2_plot = (-w2[0] * x1_plot - b2) / w2[1]\n\nplt.plot(x1_plot, x2_plot, 'k-', label='Fronteira de Decis\u00e3o')\n\nplt.title('Dados e Fronteira de Decis\u00e3o do Perceptron')\nplt.xlabel('Caracter\u00edstica 1 (X1)')\nplt.ylabel('Caracter\u00edstica 2 (X2)')\nplt.legend()\nplt.grid(True)\nplt.axis('equal')\n\nplt.show()\n</code></pre> </p>"},{"location":"perceptron/main/#resultados-finais_1","title":"Resultados Finais","text":"<p>Pesos Finais: [0.15110298, 0.01252476]</p> <p>Bias Final: -0.44</p> <p>Precis\u00e3o Final: 0.6395</p> <p>Ao utilizarmos um conjunto de dados que n\u00e3o \u00e9 linearmente separ\u00e1vel, um perceptron simples \u00e9 incapaz de encontrar uma fronteira de decis\u00e3o que classifique perfeitamente todas as amostras. Isso ocorre porque o perceptron \u00e9 um classificador linear e, quando as classes se sobrep\u00f5em, n\u00e3o h\u00e1 uma linha reta que possa separar todas as amostras de uma classe das da outra. Como resultado, o treinamento do perceptron tende a n\u00e3o convergir para uma solu\u00e7\u00e3o que classifique corretamente todas as amostras, levando a uma precis\u00e3o final inferior a 100%.</p> <p>Obs: Partes desse relat\u00f3rio foram gerados com o aux\u00edlio de IA.</p>"},{"location":"projeto/main/","title":"Main","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"roteiro1/main/","title":"Main","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Aqui vai o objetivo macro do roteiro. Por que estamos fazendo o que estamos fazendo?</p>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Instalando o MAAS:</p> sudo snap install maas --channel=3.5/Stable <p></p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro1/main/#app","title":"App","text":""},{"location":"roteiro1/main/#tarefa-1_1","title":"Tarefa 1","text":""},{"location":"roteiro1/main/#tarefa-2_1","title":"Tarefa 2","text":"<p>Exemplo de diagrama</p> <pre><code>architecture-beta\n    group api(cloud)[API]\n\n    service db(database)[Database] in api\n    service disk1(disk)[Storage] in api\n    service disk2(disk)[Storage] in api\n    service server(server)[Server] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n    disk2:T -- B:db</code></pre> <p>Mermaid</p>"},{"location":"roteiro1/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"},{"location":"roteiro2/main/","title":"Main","text":""},{"location":"roteiro2/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"roteiro2/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"roteiro3/main/","title":"Main","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> </p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"roteiro4/main/","title":"Main","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> 2025-09-21T22:44:09.712863 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>Traceback (most recent call last):\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\curl_cffi\\requests\\session.py\", line 640, in request\n    c.perform()\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\curl_cffi\\curl.py\", line 365, in perform\n    self._check_error(ret, \"perform\")\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\curl_cffi\\curl.py\", line 187, in _check_error\n    raise error\ncurl_cffi.curl.CurlError: Failed to perform, curl: (77) error setting certificate verify locations:  CAfile: C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\certifi\\cacert.pem CApath: none. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\markdown_exec\\_internal\\formatters\\python.py\", line 71, in _run_python\n    exec_python(code, code_block_id, exec_globals)\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\markdown_exec\\_internal\\formatters\\_exec_python.py\", line 8, in exec_python\n    exec(compiled, exec_globals)  # noqa: S102\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"&lt;code block: n2&gt;\", line 15, in &lt;module&gt;\n    data = info.history(period='2y')\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\utils.py\", line 92, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\base.py\", line 101, in history\n    return self._lazy_load_price_history().history(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\base.py\", line 107, in _lazy_load_price_history\n    self._price_history = PriceHistory(self._data, self.ticker, self._get_ticker_tz(timeout=10))\n                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\base.py\", line 132, in _get_ticker_tz\n    if k in self.info:\n            ^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\ticker.py\", line 163, in info\n    return self.get_info()\n           ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\base.py\", line 298, in get_info\n    data = self._quote.info\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\scrapers\\quote.py\", line 511, in info\n    self._fetch_info()\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\scrapers\\quote.py\", line 610, in _fetch_info\n    result = self._fetch(modules=modules)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\scrapers\\quote.py\", line 590, in _fetch\n    result = self._data.get_raw_json(_QUOTE_SUMMARY_URL_ + f\"/{self._symbol}\", params=params_dict)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\data.py\", line 432, in get_raw_json\n    response = self.get(url, params=params, timeout=timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\utils.py\", line 92, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\data.py\", line 370, in get\n    return self._make_request(url, request_method = self._session.get, params=params, timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\utils.py\", line 92, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\data.py\", line 391, in _make_request\n    crumb, strategy = self._get_cookie_and_crumb()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\utils.py\", line 92, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\data.py\", line 360, in _get_cookie_and_crumb\n    crumb = self._get_cookie_and_crumb_basic(timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\utils.py\", line 92, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\data.py\", line 239, in _get_cookie_and_crumb_basic\n    if not self._get_cookie_basic(timeout):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\utils.py\", line 92, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\yfinance\\data.py\", line 196, in _get_cookie_basic\n    self._session.get(\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\curl_cffi\\requests\\session.py\", line 661, in get\n    return self.request(method=\"GET\", url=url, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\curl_cffi\\requests\\session.py\", line 647, in request\n    raise error(str(e), e.code, rsp) from e\ncurl_cffi.requests.exceptions.SSLError: Failed to perform, curl: (77) error setting certificate verify locations:  CAfile: C:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\redes_neurais\\env\\Lib\\site-packages\\certifi\\cacert.pem CApath: none. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.\n</code></pre> <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"thisdocumentation/main/","title":"Main","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}